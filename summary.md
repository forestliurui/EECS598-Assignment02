## Problem and Motivation
Invention of more sophisticated machine learning models;

Availability of large datasets for tackling problems in these fields; and

Development of software platforms that enable the easy use of large amounts of computational resources for training such models on these large datasets.

TensorFlow uses a unified dataflow graph to represent both the computation in an algorithm and the state on which the algorithm operates.

Defining new layers

Refining the training algorithms

Defining new training algorithms

## Hypothesis

Defining new layers

Refining the training algorithms

Defining new training algorithms

## Solution Overview

Dataflow graphs of primitive operators

Deferred execution

Common abstraction for heterogeneous accelerators

We have also built various coordination protocols, and achieved encouraging results with synchronous replication, echoing recent results that contradict the commonly held belief that asynchronous replication is required for scalable learning.

## Limitations and Possible Improvements
we have not yet determined default policies that work well for all users.

How TensorFlow can automatically determine placements that achieve close to optimal performance on a given set of devices
